{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class RBuffer():\n",
    "    def __init__(self, maxsize, statedim, naction):\n",
    "        self.cnt = 0\n",
    "        self.maxsize = maxsize\n",
    "        self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
    "\n",
    "    def storexp(self, state, next_state, action, done, reward):\n",
    "        index = self.cnt % self.maxsize\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.done_memory[index] = 1- int(done)\n",
    "        self.cnt += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.cnt, self.maxsize)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace= False)\n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "        return states, next_states, rewards, actions, dones"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "    def fit(self, X, Z, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype= tf.float32)\n",
    "        Z = tf.convert_to_tensor(Z, dtype= tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype= tf.float32)\n",
    "        a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self(X, Z)\n",
    "            cost = tf.keras.losses.MSE(Y, action)\n",
    "            grad = tape.gradient(cost, self.trainable_variables)\n",
    "            a_opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return tf.math.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.math import logical_not, greater\n",
    "from tensorflow import gather\n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        t = gather(inputstate, [0], axis=1)\n",
    "        x1 = gather(inputstate, [1], axis=1) * 100\n",
    "        x2 = gather(inputstate, [2], axis=1) * 50\n",
    "        cost1 = x1 - x2 - 100.\n",
    "        cost2 = x1 - 100. + x2\n",
    "        cost1 *= tf.cast(greater(cost1, 0.), tf.float32) + 1.\n",
    "        cost1 *= cost1\n",
    "        cost2 *= cost2\n",
    "        cost = (cost1 + cost2) / 2\n",
    "        #print(t, cost, tf.norm(action, axis=1)**2)\n",
    "        return self.v(self.f2(self.f1(tf.concat([t, -cost, -tf.reshape(tf.norm(action, axis=1) ** 2, (128, 1))], axis=1))))\n",
    "    def fit(self, X, Z, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype= tf.float32)\n",
    "        Z = tf.convert_to_tensor(Z, dtype= tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype= tf.float32)\n",
    "        a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self(X, Z)\n",
    "            cost = tf.keras.losses.MSE(Y, action)\n",
    "            grad = tape.gradient(cost, self.trainable_variables)\n",
    "            a_opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return tf.math.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.1)\n",
    "        self.f1 = tf.keras.layers.Dense(40, kernel_initializer=initializer, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(40, kernel_initializer=initializer, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)\n",
    "        return x\n",
    "    def fit(self, X, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype= tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype= tf.float32)\n",
    "        a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self(X)\n",
    "            cost = tf.keras.losses.MSE(Y, action)\n",
    "            grad = tape.gradient(cost, self.trainable_variables)\n",
    "            a_opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return tf.math.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_action= 2):\n",
    "        self.actor_main = Actor(n_action)\n",
    "        self.actor_target = Actor(n_action)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_main2 = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target2 = Critic()\n",
    "        self.batch_size = 128\n",
    "        self.n_actions = 2\n",
    "        self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        # self.actor_target = tf.keras.optimizers.Adam(.001)\n",
    "        self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
    "        # self.critic_target = tf.keras.optimizers.Adam(.002)\n",
    "        self.memory = RBuffer(100000, [3], n_action)\n",
    "        self.trainstep = 0\n",
    "        #self.replace = 5\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = -10\n",
    "        self.max_action = 10\n",
    "        self.actor_update_steps = 2\n",
    "        self.warmup = 200\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt1)\n",
    "        self.critic_target2.compile(optimizer=self.c_opt2)\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def savexp(self,state, next_state, action, done, reward):\n",
    "        self.memory.storexp(state, next_state, action, done, reward)\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "        weights3 = []\n",
    "        targets3 = self.critic_target2.weights\n",
    "        for i, weight in enumerate(self.critic_main2.weights):\n",
    "            weights3.append(weight * tau + targets3[i]*(1-tau))\n",
    "        self.critic_target2.set_weights(weights3)\n",
    "\n",
    "  \n",
    "    def train(self):\n",
    "        if self.memory.cnt < self.batch_size:\n",
    "            return\n",
    "        states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "        #dones = tf.convert_to_tensor(dones, dtype= tf.bool)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(next_states)\n",
    "            target_actions += tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = self.max_action * (tf.clip_by_value(target_actions, self.min_action, self.max_action))\n",
    "\n",
    "\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(next_states, target_actions), 1)\n",
    "            target_next_state_values2 = tf.squeeze(self.critic_target2(next_states, target_actions), 1)\n",
    "\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            critic_value2 = tf.squeeze(self.critic_main2(states, actions), 1)\n",
    "\n",
    "            next_state_target_value = tf.math.minimum(target_next_state_values, target_next_state_values2)\n",
    "\n",
    "            target_values = rewards + self.gamma * next_state_target_value * dones\n",
    "            critic_loss1 = tf.keras.losses.MSE(target_values, critic_value)\n",
    "            critic_loss2 = tf.keras.losses.MSE(target_values, critic_value2)\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic_main2.trainable_variables)\n",
    "        self.c_opt1.apply_gradients(zip(grads1, self.critic_main.trainable_variables))\n",
    "        self.c_opt2.apply_gradients(zip(grads2, self.critic_main2.trainable_variables))\n",
    "        self.trainstep +=1\n",
    "        if self.trainstep % self.actor_update_steps == 0:\n",
    "            with tf.GradientTape() as tape3:\n",
    "                new_policy_actions = self.actor_main(states)\n",
    "                actor_loss = -self.critic_main(states, new_policy_actions)\n",
    "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "            grads3 = tape3.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "            self.a_opt.apply_gradients(zip(grads3, self.actor_main.trainable_variables))\n",
    "\n",
    "        #if self.trainstep % self.replace == 0:\n",
    "        self.update_target()\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if self.trainstep > self.warmup:\n",
    "            evaluate = True\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
    "        return actions[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from Angrybird import AngryBird\n",
    "scale = np.array([10., 100., 50.])\n",
    "env = AngryBird()\n",
    "episods = 1000\n",
    "states = []\n",
    "actions = []\n",
    "costs = []\n",
    "for s in range(episods):\n",
    "    state = env.reset()\n",
    "    cost = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        hand_made_state = state / scale\n",
    "        if state[0] == 11.:\n",
    "            action = np.zeros(2)\n",
    "        else:\n",
    "            action = main(state[0], state[1:])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Update dataset\n",
    "        states.append(hand_made_state)\n",
    "        actions.append(action)\n",
    "        costs.append(reward)\n",
    "        state = next_state\n",
    "states = np.array(states)\n",
    "actions = np.array(actions)\n",
    "costs = np.array(costs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.device('GPU:0'):\n",
    "    a = 50\n",
    "    while a > 1.25:\n",
    "        a = agent.actor_main.fit(states, actions).numpy()\n",
    "        agent.actor_target.fit(states, actions)\n",
    "        print(a, end=\" \")\n",
    "    a = 50\n",
    "    print()\n",
    "    while a > 0.01:\n",
    "        a = agent.critic_main.fit(states, actions, costs).numpy()\n",
    "        agent.critic_main2.fit(states, actions, costs).numpy()\n",
    "        agent.critic_target.fit(states, actions, costs).numpy()\n",
    "        agent.critic_target2.fit(states, actions, costs).numpy()\n",
    "        print(a, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seconde, position):\n",
    "    return np.array([-(position[0]-10.*seconde),-(position[1]-(20.*seconde-2*seconde** 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 0 steps is -203.66074148644202 and avg reward is -203.66074148644202\n",
      "total reward after 50 steps is -23.204048400815886 and avg reward is -113.43239494362895\n",
      "total reward after 100 steps is -69.05250564844638 and avg reward is -98.63909851190142\n",
      "total reward after 150 steps is -123.7394603692791 and avg reward is -104.91418897624584\n",
      "total reward after 200 steps is -12.602475882822269 and avg reward is -86.45184635756112\n",
      "total reward after 250 steps is -664.8414169278736 and avg reward is -182.85010811927987\n",
      "total reward after 300 steps is -293.2870137029711 and avg reward is -198.62680891695004\n",
      "total reward after 350 steps is -9.241229053163902 and avg reward is -174.9536114339768\n",
      "total reward after 400 steps is -495.81024413466344 and avg reward is -210.60434840071974\n",
      "total reward after 450 steps is -402.58453241802886 and avg reward is -229.80236680245065\n",
      "total reward after 500 steps is -218.5981947520105 and avg reward is -228.7838057069561\n",
      "total reward after 550 steps is -17.506155009684782 and avg reward is -211.17733481551682\n",
      "total reward after 600 steps is -80.8940045218518 and avg reward is -201.1555401775426\n",
      "total reward after 650 steps is -130.97041188041402 and avg reward is -196.14231672774767\n",
      "total reward after 700 steps is -266.5013946536466 and avg reward is -200.8329219228076\n",
      "total reward after 750 steps is -130.19135740469875 and avg reward is -196.4178241404258\n",
      "total reward after 800 steps is -378.19985286372923 and avg reward is -207.1108846535613\n",
      "total reward after 850 steps is -1324.2896593156606 and avg reward is -269.17637213478906\n",
      "total reward after 900 steps is -464.14948219798305 and avg reward is -279.438114769694\n",
      "total reward after 950 steps is -389.8520250259048 and avg reward is -284.9588102825045\n",
      "total reward after 1000 steps is -365.35931801420134 and avg reward is -288.7874058887758\n",
      "total reward after 1050 steps is -613.5635649607328 and avg reward is -303.5499585738648\n",
      "total reward after 1100 steps is -437.30667373807506 and avg reward is -309.3654679288304\n",
      "total reward after 1150 steps is -426.8305994603837 and avg reward is -314.2598484093118\n",
      "total reward after 1200 steps is -373.7085641348817 and avg reward is -316.6377970383346\n",
      "total reward after 1250 steps is -430.4117271914489 and avg reward is -321.013717428839\n",
      "total reward after 1300 steps is -467.0212724116651 and avg reward is -326.42140465042513\n",
      "total reward after 1350 steps is -409.1691340443796 and avg reward is -329.37668070020925\n",
      "total reward after 1400 steps is -699.9110559608787 and avg reward is -342.1537281229909\n",
      "total reward after 1450 steps is -511.24518904968613 and avg reward is -347.7901101538808\n",
      "total reward after 1500 steps is -391.07757578057954 and avg reward is -349.18648001280656\n",
      "total reward after 1550 steps is -393.96007275477047 and avg reward is -350.5856547859929\n",
      "total reward after 1600 steps is -501.25818962613755 and avg reward is -355.1514891750882\n",
      "total reward after 1650 steps is -951.385316673155 and avg reward is -372.68777821914904\n",
      "total reward after 1700 steps is -394.17855262868545 and avg reward is -373.3018003451358\n",
      "total reward after 1750 steps is -393.1585286433165 and avg reward is -373.85337613119634\n",
      "total reward after 1800 steps is -407.4274206423711 and avg reward is -374.7607827396065\n",
      "total reward after 1850 steps is -385.75835690680026 and avg reward is -375.05019258611156\n",
      "total reward after 1900 steps is -541.194613350052 and avg reward is -379.3103059390331\n",
      "total reward after 1950 steps is -389.24013882853455 and avg reward is -379.5585517612707\n",
      "total reward after 2000 steps is -378.75359178747334 and avg reward is -379.5389185911781\n",
      "total reward after 2050 steps is -517.2490238696209 and avg reward is -382.8177306216172\n",
      "total reward after 2100 steps is -459.02062089642845 and avg reward is -384.5898908605663\n",
      "total reward after 2150 steps is -409.1170263661776 and avg reward is -385.1473257584211\n",
      "total reward after 2200 steps is -456.5397133870771 and avg reward is -386.73382326128007\n",
      "total reward after 2250 steps is -522.9201522759013 and avg reward is -389.6943956311631\n",
      "total reward after 2300 steps is -449.5752349530019 and avg reward is -390.96845604226604\n",
      "total reward after 2350 steps is -407.50810876436725 and avg reward is -391.3130321406432\n",
      "total reward after 2400 steps is -448.82087624313306 and avg reward is -392.4866616121226\n",
      "total reward after 2450 steps is -835.1169756952524 and avg reward is -401.3392678937851\n",
      "total reward after 2500 steps is -407.57004006851196 and avg reward is -401.46143989721116\n",
      "total reward after 2550 steps is -421.30379317320876 and avg reward is -401.84302361405724\n",
      "total reward after 2600 steps is -395.7434014844357 and avg reward is -401.7279364040644\n",
      "total reward after 2650 steps is -1711.788145975235 and avg reward is -425.9883106553824\n",
      "total reward after 2700 steps is -451.1325033538592 and avg reward is -426.4454777953547\n",
      "total reward after 2750 steps is -419.84458302262044 and avg reward is -426.327604674413\n",
      "total reward after 2800 steps is -962.1295079825368 and avg reward is -435.7276380657836\n",
      "total reward after 2850 steps is -407.6866179306435 and avg reward is -435.24417220138463\n",
      "total reward after 2900 steps is -604.7039577229516 and avg reward is -438.11637195598746\n",
      "total reward after 2950 steps is -373.2254758055444 and avg reward is -437.0348570201468\n",
      "total reward after 3000 steps is -609.8602527540734 and avg reward is -439.8680602288997\n",
      "total reward after 3050 steps is -411.0468084283053 and avg reward is -439.40320132889013\n",
      "total reward after 3100 steps is -399.7816469094851 and avg reward is -438.7742877666774\n",
      "total reward after 3150 steps is -475.00880782333365 and avg reward is -439.3404521425625\n",
      "total reward after 3200 steps is -473.95425782319853 and avg reward is -439.87297222995693\n",
      "total reward after 3250 steps is -581.9476212657497 and avg reward is -442.02561842746894\n",
      "total reward after 3300 steps is -795.2088863406535 and avg reward is -447.2970104858747\n",
      "total reward after 3350 steps is -379.8305040050384 and avg reward is -446.3048559788036\n",
      "total reward after 3400 steps is -491.8027511059309 and avg reward is -446.9642457632547\n",
      "total reward after 3450 steps is -607.630775536098 and avg reward is -449.25948190286675\n",
      "total reward after 3500 steps is -1032.3886886183554 and avg reward is -457.4725693213947\n",
      "total reward after 3550 steps is -377.72273170396727 and avg reward is -456.3649326878194\n",
      "total reward after 3600 steps is -572.0560424830726 and avg reward is -457.949742411042\n",
      "total reward after 3650 steps is -429.27940536670087 and avg reward is -457.56230542395633\n",
      "total reward after 3700 steps is -407.893108864373 and avg reward is -456.90004946982856\n",
      "total reward after 3750 steps is -430.87141835152386 and avg reward is -456.5575674814298\n",
      "total reward after 3800 steps is -399.32254076342133 and avg reward is -455.8142554461311\n",
      "total reward after 3850 steps is -561.758741064456 and avg reward is -457.1725180822634\n",
      "total reward after 3900 steps is -525.427456772511 and avg reward is -458.0365046479627\n",
      "total reward after 3950 steps is -407.7806038288978 and avg reward is -457.4083058877244\n",
      "total reward after 4000 steps is -395.0366626859456 and avg reward is -456.63828560128275\n",
      "total reward after 4050 steps is -816.142237184197 and avg reward is -461.0224801327817\n",
      "total reward after 4100 steps is -552.6960715383436 and avg reward is -462.1269812340536\n",
      "total reward after 4150 steps is -581.1517006437538 and avg reward is -463.54394217940717\n",
      "total reward after 4200 steps is -474.54576056207657 and avg reward is -463.6733753368503\n",
      "total reward after 4250 steps is -543.1955589087559 and avg reward is -464.5980518900121\n",
      "total reward after 4300 steps is -1479.7901028640413 and avg reward is -476.2669260391388\n",
      "total reward after 4350 steps is -670.934236658022 and avg reward is -478.4790545688988\n",
      "total reward after 4400 steps is -393.86267379929416 and avg reward is -477.528308717555\n",
      "total reward after 4450 steps is -384.3596841513907 and avg reward is -476.4931017779309\n",
      "total reward after 4500 steps is -446.71711883415924 and avg reward is -476.1658931741532\n",
      "total reward after 4550 steps is -435.1854558610246 and avg reward is -475.72045363814095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 4600 steps is -408.2278854472787 and avg reward is -474.9947270984542\n",
      "total reward after 4650 steps is -394.31233958872053 and avg reward is -474.1364038270741\n",
      "total reward after 4700 steps is -597.3105428944348 and avg reward is -475.43297371199367\n",
      "total reward after 4750 steps is -462.9240003243933 and avg reward is -475.3026719058727\n",
      "total reward after 4800 steps is -395.4949614313009 and avg reward is -474.479912004073\n",
      "total reward after 4850 steps is -413.44806641192133 and avg reward is -473.85713806945915\n",
      "total reward after 4900 steps is -377.5447645233337 and avg reward is -472.88428581141756\n",
      "total reward after 4950 steps is -396.44015264605855 and avg reward is -472.1198444797639\n",
      "Saving model...\n",
      "total reward after 5000 steps is -885.8868801620788 and avg reward is -478.94210586652036\n",
      "total reward after 5050 steps is -434.1843310242894 and avg reward is -483.0519086927551\n",
      "total reward after 5100 steps is -385.64313242843946 and avg reward is -486.217814960555\n",
      "total reward after 5150 steps is -412.38689371240446 and avg reward is -489.1042892939863\n",
      "total reward after 5200 steps is -555.5878487685817 and avg reward is -494.5341430228439\n",
      "total reward after 5250 steps is -384.98611905229643 and avg reward is -491.7355900440881\n",
      "total reward after 5300 steps is -403.94430497484655 and avg reward is -492.8421629568069\n",
      "total reward after 5350 steps is -389.05357913443515 and avg reward is -496.64028645761954\n",
      "total reward after 5400 steps is -419.60905901617093 and avg reward is -495.87827460643456\n",
      "total reward after 5450 steps is -427.12420834065034 and avg reward is -496.12367136566087\n",
      "total reward after 5500 steps is -393.80057658129215 and avg reward is -497.87569518395367\n",
      "total reward after 5550 steps is -402.67292958525695 and avg reward is -501.72736292970933\n",
      "total reward after 5600 steps is -471.9957631682326 and avg reward is -505.6383805161732\n",
      "total reward after 5650 steps is -373.5421561890684 and avg reward is -508.06409795925975\n",
      "total reward after 5700 steps is -387.82966983338076 and avg reward is -509.2773807110571\n",
      "total reward after 5750 steps is -409.17619079350493 and avg reward is -512.0672290449452\n",
      "total reward after 5800 steps is -424.3584906230163 and avg reward is -512.5288154225381\n",
      "total reward after 5850 steps is -517.0667000954921 and avg reward is -504.4565858303364\n",
      "total reward after 5900 steps is -586.305610319579 and avg reward is -505.67814711155233\n",
      "total reward after 5950 steps is -468.8641446637701 and avg reward is -506.468268307931\n",
      "total reward after 6000 steps is -756.7983882636393 and avg reward is -510.38265901042524\n",
      "total reward after 6050 steps is -496.0793071447302 and avg reward is -509.2078164322653\n",
      "total reward after 6100 steps is -441.8834579098717 and avg reward is -509.2535842739833\n",
      "total reward after 6150 steps is -1684.762190637134 and avg reward is -521.8329001857508\n",
      "total reward after 6200 steps is -523.0304130741672 and avg reward is -523.3261186751436\n",
      "total reward after 6250 steps is -545.5407560937231 and avg reward is -524.4774089641663\n",
      "total reward after 6300 steps is -541.1680713460959 and avg reward is -525.2188769535106\n",
      "total reward after 6350 steps is -389.6384065370056 and avg reward is -525.023569678437\n",
      "total reward after 6400 steps is -659.0351344425128 and avg reward is -524.6148104632533\n",
      "total reward after 6450 steps is -426.2230804618762 and avg reward is -523.7645893773752\n",
      "total reward after 6500 steps is -476.66326350755 and avg reward is -524.6204462546449\n",
      "total reward after 6550 steps is -415.47799090062506 and avg reward is -524.8356254361034\n",
      "total reward after 6600 steps is -428.7781247155767 and avg reward is -524.1108247869978\n",
      "total reward after 6650 steps is -464.8636074989895 and avg reward is -519.2456076952562\n",
      "total reward after 6700 steps is -430.4339865992064 and avg reward is -519.6081620349614\n",
      "total reward after 6750 steps is -607.5632792022113 and avg reward is -521.7522095405503\n",
      "total reward after 6800 steps is -409.0006129242249 and avg reward is -521.7679414633689\n",
      "total reward after 6850 steps is -402.71582058365254 and avg reward is -521.9375161001374\n",
      "total reward after 6900 steps is -384.7445412904526 and avg reward is -520.3730153795414\n",
      "total reward after 6950 steps is -415.23453505074986 and avg reward is -520.6329593417635\n",
      "total reward after 7000 steps is -429.31058975800534 and avg reward is -521.1385293214689\n",
      "total reward after 7050 steps is -402.0287216215155 and avg reward is -519.9863262989877\n",
      "total reward after 7100 steps is -513.440699665942 and avg reward is -520.5305270866829\n",
      "total reward after 7150 steps is -469.48699964315665 and avg reward is -521.1342268194527\n",
      "total reward after 7200 steps is -422.02699086715165 and avg reward is -520.7890995942536\n",
      "total reward after 7250 steps is -419.3753905484419 and avg reward is -519.7536519769789\n",
      "total reward after 7300 steps is -592.321163533057 and avg reward is -521.1811112627795\n",
      "total reward after 7350 steps is -495.8336422531024 and avg reward is -522.0643665976668\n",
      "total reward after 7400 steps is -583.9606730220231 and avg reward is -523.4157645654557\n",
      "total reward after 7450 steps is -696.722380660452 and avg reward is -522.0318186151077\n",
      "total reward after 7500 steps is -408.28146840043735 and avg reward is -522.038932898427\n",
      "total reward after 7550 steps is -410.8688615064921 and avg reward is -521.9345835817599\n",
      "total reward after 7600 steps is -561.5541632116295 and avg reward is -523.5926911990317\n",
      "total reward after 7650 steps is -388.45480282288406 and avg reward is -510.3593577675082\n",
      "total reward after 7700 steps is -601.8314135387558 and avg reward is -511.8663468693571\n",
      "total reward after 7750 steps is -437.8788861792984 and avg reward is -512.0466899009239\n",
      "total reward after 7800 steps is -390.80736294889726 and avg reward is -506.33346845058753\n",
      "total reward after 7850 steps is -613.8586586708925 and avg reward is -508.3951888579901\n",
      "total reward after 7900 steps is -466.93142720530784 and avg reward is -507.01746355281364\n",
      "total reward after 7950 steps is -401.83919194619045 and avg reward is -507.30360071422007\n",
      "total reward after 8000 steps is -420.06435678945365 and avg reward is -505.4056417545738\n",
      "total reward after 8050 steps is -470.9453759460932 and avg reward is -506.00462742975174\n",
      "total reward after 8100 steps is -593.7568544444377 and avg reward is -507.94437950510127\n",
      "total reward after 8150 steps is -389.215497873343 and avg reward is -507.0864464056013\n",
      "total reward after 8200 steps is -617.4175027489342 and avg reward is -508.5210788548586\n",
      "total reward after 8250 steps is -379.71621478787586 and avg reward is -506.49876479007986\n",
      "total reward after 8300 steps is -475.45696445099554 and avg reward is -503.3012455711834\n",
      "total reward after 8350 steps is -404.82656119067684 and avg reward is -503.55120614303974\n",
      "total reward after 8400 steps is -571.8118384696669 and avg reward is -504.3512970166771\n",
      "total reward after 8450 steps is -496.36474845675156 and avg reward is -503.2386367458836\n",
      "total reward after 8500 steps is -792.9616903280112 and avg reward is -500.84436676298014\n",
      "total reward after 8550 steps is -409.53651494411315 and avg reward is -501.1625045953816\n",
      "total reward after 8600 steps is -650.1914388595308 and avg reward is -501.94385855914624\n",
      "total reward after 8650 steps is -402.87635850608876 and avg reward is -501.6798280905402\n",
      "total reward after 8700 steps is -969.5946127674522 and avg reward is -507.296843129571\n",
      "total reward after 8750 steps is -1777.2724355540722 and avg reward is -520.7608533015964\n",
      "total reward after 8800 steps is -494.95161156599795 and avg reward is -521.7171440096222\n",
      "total reward after 8850 steps is -1022.8598644645128 and avg reward is -526.3281552436229\n",
      "total reward after 8900 steps is -376.1291885322365 and avg reward is -524.83517256122\n",
      "total reward after 8950 steps is -836.3932071797665 and avg reward is -529.1212985947287\n",
      "total reward after 9000 steps is -409.063595320932 and avg reward is -529.2615679210786\n",
      "total reward after 9050 steps is -755.2872331984174 and avg reward is -528.6530178812208\n",
      "total reward after 9100 steps is -424.3928100156244 and avg reward is -527.3699852659937\n",
      "total reward after 9150 steps is -692.6271017730855 and avg reward is -528.484739277287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 9200 steps is -431.5834592896543 and avg reward is -528.0551162645628\n",
      "total reward after 9250 steps is -645.3385737385149 and avg reward is -529.0765464128604\n",
      "total reward after 9300 steps is -596.1831702975992 and avg reward is -520.2404770871959\n",
      "total reward after 9350 steps is -670.878249703464 and avg reward is -520.2399172176503\n",
      "total reward after 9400 steps is -650.4115417929681 and avg reward is -522.8054058975871\n",
      "total reward after 9450 steps is -427.01434180566037 and avg reward is -523.2319524741297\n",
      "total reward after 9500 steps is -711.2452546022748 and avg reward is -525.8772338318109\n",
      "total reward after 9550 steps is -565.7043041895495 and avg reward is -527.1824223150962\n",
      "total reward after 9600 steps is -446.73057604272014 and avg reward is -527.5674492210505\n",
      "total reward after 9650 steps is -416.68447849560727 and avg reward is -527.7911706101195\n",
      "total reward after 9700 steps is -750.8223092759313 and avg reward is -529.3262882739343\n",
      "total reward after 9750 steps is -1230.0424939497238 and avg reward is -536.9974732101876\n",
      "total reward after 9800 steps is -648.8898255778047 and avg reward is -539.5314218516527\n",
      "total reward after 9850 steps is -413.7589990569959 and avg reward is -539.5345311781034\n",
      "total reward after 9900 steps is -679.7549980065171 and avg reward is -542.5566335129354\n",
      "total reward after 9950 steps is -763.2473390937487 and avg reward is -546.2247053774122\n",
      "Saving model...\n",
      "total reward after 10000 steps is -2534.472825782156 and avg reward is -562.7105648336129\n",
      "total reward after 10050 steps is -453.095394253246 and avg reward is -562.8996754659025\n",
      "total reward after 10100 steps is -700.9041022685892 and avg reward is -566.0522851643041\n",
      "total reward after 10150 steps is -398.2000792986954 and avg reward is -565.910417020167\n",
      "total reward after 10200 steps is -1560.4255701023023 and avg reward is -575.9587942335041\n",
      "total reward after 10250 steps is -440.02995849852414 and avg reward is -576.5092326279664\n",
      "total reward after 10300 steps is -537.3472299253634 and avg reward is -577.8432618774715\n",
      "total reward after 10350 steps is -409.6127502537738 and avg reward is -578.0488535886649\n",
      "total reward after 10400 steps is -364.85310860405747 and avg reward is -577.5012940845438\n",
      "total reward after 10450 steps is -384.38184279814277 and avg reward is -577.0738704291188\n",
      "total reward after 10500 steps is -498.6397568247668 and avg reward is -578.1222622315536\n",
      "total reward after 10550 steps is -1343.3781464068982 and avg reward is -587.52931439977\n",
      "total reward after 10600 steps is -730.2979393470041 and avg reward is -590.1123361615577\n",
      "total reward after 10650 steps is -386.88855091773155 and avg reward is -590.2458001088443\n",
      "total reward after 10700 steps is -608.3525612069836 and avg reward is -592.4510290225803\n",
      "total reward after 10750 steps is -399.92512047379194 and avg reward is -592.3585183193832\n",
      "total reward after 10800 steps is -392.23309971904285 and avg reward is -592.0372644103434\n",
      "total reward after 10850 steps is -819.4208221341969 and avg reward is -595.0608056307304\n",
      "total reward after 10900 steps is -490.4389206852217 and avg reward is -594.1021387343869\n",
      "total reward after 10950 steps is -443.81983887445074 and avg reward is -593.8516956764937\n",
      "total reward after 11000 steps is -584.1744349555365 and avg reward is -592.1254561434126\n",
      "total reward after 11050 steps is -412.8956974589589 and avg reward is -591.2936200465549\n",
      "total reward after 11100 steps is -538.3706937365961 and avg reward is -592.2584924048223\n",
      "total reward after 11150 steps is -617.0268996620912 and avg reward is -581.5811394950717\n",
      "total reward after 11200 steps is -399.6361908860863 and avg reward is -580.347197273191\n",
      "total reward after 11250 steps is -413.15699053655396 and avg reward is -579.0233596176192\n",
      "total reward after 11300 steps is -401.8839015816908 and avg reward is -577.6305179199752\n",
      "total reward after 11350 steps is -462.49551864738396 and avg reward is -578.359089041079\n",
      "total reward after 11400 steps is -412.67617233696285 and avg reward is -575.8954994200235\n",
      "total reward after 11450 steps is -696.5254222729205 and avg reward is -578.5985228381339\n",
      "total reward after 11500 steps is -386.33306721613576 and avg reward is -577.6952208752198\n",
      "total reward after 11550 steps is -693.0857006061709 and avg reward is -580.4712979722752\n",
      "total reward after 11600 steps is -475.8685609103661 and avg reward is -580.9422023342231\n",
      "total reward after 11650 steps is -429.2923698493831 and avg reward is -580.586489957727\n",
      "total reward after 11700 steps is -631.8082949752683 and avg reward is -582.6002330414876\n",
      "total reward after 11750 steps is -383.13450666730796 and avg reward is -580.3559453161387\n",
      "total reward after 11800 steps is -455.8085168446542 and avg reward is -580.8240243553431\n",
      "total reward after 11850 steps is -550.6363749352636 and avg reward is -582.3032298988592\n",
      "total reward after 11900 steps is -416.87540804633215 and avg reward is -582.6245385664179\n",
      "total reward after 11950 steps is -626.4159921590758 and avg reward is -584.7363531375012\n",
      "total reward after 12000 steps is -481.94112327244306 and avg reward is -585.2626584726455\n",
      "total reward after 12050 steps is -720.293353554117 and avg reward is -588.4453047919716\n",
      "total reward after 12100 steps is -454.46117619361047 and avg reward is -587.8555095572483\n",
      "total reward after 12150 steps is -800.9613997598833 and avg reward is -591.1702535584155\n",
      "total reward after 12200 steps is -605.8028529201274 and avg reward is -593.0080121789453\n",
      "total reward after 12250 steps is -464.9441512078388 and avg reward is -593.4636997855392\n",
      "total reward after 12300 steps is -513.5470888663993 and avg reward is -592.6759590388726\n",
      "total reward after 12350 steps is -405.780076710776 and avg reward is -591.7754233834494\n",
      "total reward after 12400 steps is -523.9427963733781 and avg reward is -591.1752446169629\n",
      "total reward after 12450 steps is -523.5030017658712 and avg reward is -589.443050828017\n",
      "total reward after 12500 steps is -550.6975766679109 and avg reward is -590.8672119106919\n",
      "total reward after 12550 steps is -430.3759569535466 and avg reward is -591.0622828651624\n",
      "total reward after 12600 steps is -527.488087201157 and avg reward is -590.7216221050577\n",
      "total reward after 12650 steps is -445.47734635055326 and avg reward is -591.2918475403344\n",
      "total reward after 12700 steps is -547.338084869058 and avg reward is -590.7469142536374\n",
      "total reward after 12750 steps is -453.8439119436895 and avg reward is -590.9065645112813\n",
      "total reward after 12800 steps is -376.3796740705843 and avg reward is -590.7622876224982\n",
      "total reward after 12850 steps is -470.26593447675805 and avg reward is -589.3263603805568\n",
      "total reward after 12900 steps is -672.6177187545267 and avg reward is -591.383223296049\n",
      "total reward after 12950 steps is -399.9245386631543 and avg reward is -591.3640767632186\n",
      "total reward after 13000 steps is -435.11776818176327 and avg reward is -591.5146108771416\n",
      "total reward after 13050 steps is -999.0492423483186 and avg reward is -596.7956495411639\n",
      "total reward after 13100 steps is -655.7259751980993 and avg reward is -597.4153407487006\n",
      "total reward after 13150 steps is -922.1319356512244 and avg reward is -602.7445051264793\n",
      "total reward after 13200 steps is -628.5657966760709 and avg reward is -602.8559880657507\n",
      "total reward after 13250 steps is -404.3215595563703 and avg reward is -603.1020415134357\n",
      "total reward after 13300 steps is -504.0689571852997 and avg reward is -603.3881614407787\n",
      "total reward after 13350 steps is -449.70014401594375 and avg reward is -603.8368972690314\n",
      "total reward after 13400 steps is -429.19698050518866 and avg reward is -602.4107486893867\n",
      "total reward after 13450 steps is -409.43662659402605 and avg reward is -601.5414674707594\n",
      "total reward after 13500 steps is -409.97214391615455 and avg reward is -597.7115720066408\n",
      "total reward after 13550 steps is -422.5678847592268 and avg reward is -597.8418857047919\n",
      "total reward after 13600 steps is -382.4580127958261 and avg reward is -595.1645514441549\n",
      "total reward after 13650 steps is -961.0946313059443 and avg reward is -600.7467341721534\n",
      "total reward after 13700 steps is -488.6004810633285 and avg reward is -595.9367928551123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 13750 steps is -546.7299983494588 and avg reward is -583.6313684830661\n",
      "total reward after 13800 steps is -386.9342700884399 and avg reward is -582.5511950682906\n",
      "total reward after 13850 steps is -405.1713750913928 and avg reward is -576.3743101745594\n",
      "total reward after 13900 steps is -710.1465664504722 and avg reward is -579.7144839537417\n",
      "total reward after 13950 steps is -453.0201528420688 and avg reward is -575.8807534103646\n",
      "total reward after 14000 steps is -684.321311634692 and avg reward is -578.6333305735022\n",
      "total reward after 14050 steps is -636.9625040944502 and avg reward is -577.4500832824626\n",
      "total reward after 14100 steps is -446.9323959723551 and avg reward is -577.6754791420299\n",
      "total reward after 14150 steps is -383.89087844263236 and avg reward is -574.5881169087254\n",
      "total reward after 14200 steps is -662.8386285337685 and avg reward is -576.9006686011667\n",
      "total reward after 14250 steps is -409.65648811915906 and avg reward is -574.543847744973\n",
      "total reward after 14300 steps is -451.6716436813081 and avg reward is -573.0987324788101\n",
      "total reward after 14350 steps is -622.7525812986462 and avg reward is -572.617475794762\n",
      "total reward after 14400 steps is -497.03690365069474 and avg reward is -571.0837294133393\n",
      "total reward after 14450 steps is -690.8655353394483 and avg reward is -573.7222413486772\n",
      "total reward after 14500 steps is -451.7571434312844 and avg reward is -571.1273602369673\n",
      "total reward after 14550 steps is -408.68799167347953 and avg reward is -569.5571971118064\n",
      "total reward after 14600 steps is -404.9366724053609 and avg reward is -569.1392580754328\n",
      "total reward after 14650 steps is -843.0685340409431 and avg reward is -573.4030986308861\n",
      "total reward after 14700 steps is -401.651437694094 and avg reward is -569.9113899150677\n",
      "total reward after 14750 steps is -655.2367634101788 and avg reward is -564.1633326096724\n",
      "total reward after 14800 steps is -397.65223764076654 and avg reward is -561.6509567303019\n",
      "total reward after 14850 steps is -393.0850353899516 and avg reward is -561.4442170936316\n",
      "total reward after 14900 steps is -424.81366789655715 and avg reward is -558.894803792532\n",
      "total reward after 14950 steps is -386.5320699553308 and avg reward is -555.1276511011479\n",
      "Saving model...\n",
      "total reward after 15000 steps is -420.40167218608786 and avg reward is -533.9869395651872\n",
      "total reward after 15050 steps is -623.4382854985495 and avg reward is -535.6903684776403\n",
      "total reward after 15100 steps is -460.06596409549593 and avg reward is -533.2819870959094\n",
      "total reward after 15150 steps is -463.7597816864004 and avg reward is -533.9375841197864\n",
      "total reward after 15200 steps is -582.225450691886 and avg reward is -524.1555829256821\n",
      "total reward after 15250 steps is -400.018960354257 and avg reward is -523.7554729442395\n",
      "total reward after 15300 steps is -423.4163416145624 and avg reward is -522.6161640611314\n",
      "total reward after 15350 steps is -440.5871988544477 and avg reward is -522.9259085471381\n",
      "total reward after 15400 steps is -834.7437423028633 and avg reward is -527.6248148841262\n",
      "total reward after 15450 steps is -555.8757077257758 and avg reward is -529.3397535334025\n",
      "total reward after 15500 steps is -412.5342540531333 and avg reward is -528.4786985056862\n",
      "total reward after 15550 steps is -410.74901645747184 and avg reward is -519.1524072061919\n",
      "total reward after 15600 steps is -411.15634633814295 and avg reward is -515.9609912761034\n",
      "total reward after 15650 steps is -436.0175472918736 and avg reward is -516.4522812398448\n",
      "total reward after 15700 steps is -490.4523943290916 and avg reward is -515.2732795710658\n",
      "total reward after 15750 steps is -390.6138168632615 and avg reward is -515.1801665349606\n",
      "total reward after 15800 steps is -390.0144397240248 and avg reward is -515.1579799350102\n",
      "total reward after 15850 steps is -398.8483300462522 and avg reward is -510.95225501413086\n",
      "total reward after 15900 steps is -410.6618361517102 and avg reward is -510.15448416879576\n",
      "total reward after 15950 steps is -391.0322800740018 and avg reward is -509.62660858079136\n",
      "total reward after 16000 steps is -640.0436489169133 and avg reward is -510.18530072040517\n",
      "total reward after 16050 steps is -591.0745536445792 and avg reward is -511.96708928226127\n",
      "total reward after 16100 steps is -468.12258505205966 and avg reward is -511.26460819541586\n",
      "total reward after 16150 steps is -880.1396078902753 and avg reward is -513.8957352776978\n",
      "total reward after 16200 steps is -568.919576947021 and avg reward is -515.5885691383071\n",
      "total reward after 16250 steps is -702.8896971308899 and avg reward is -518.4858962042505\n",
      "total reward after 16300 steps is -419.72489702971967 and avg reward is -518.6643061587308\n",
      "total reward after 16350 steps is -470.1872597370042 and avg reward is -518.741223569627\n",
      "total reward after 16400 steps is -497.50428128873557 and avg reward is -519.5895046591446\n",
      "total reward after 16450 steps is -392.02626317811365 and avg reward is -516.5445130681966\n",
      "total reward after 16500 steps is -496.06444227342257 and avg reward is -517.6418268187695\n",
      "total reward after 16550 steps is -403.3776121468875 and avg reward is -514.7447459341766\n",
      "total reward after 16600 steps is -393.9732409187699 and avg reward is -513.9257927342607\n",
      "total reward after 16650 steps is -392.7336622689156 and avg reward is -513.5602056584561\n",
      "total reward after 16700 steps is -706.3001070129336 and avg reward is -514.3051237788327\n",
      "total reward after 16750 steps is -1191.8029244947088 and avg reward is -522.3918079571067\n",
      "total reward after 16800 steps is -496.8512363536681 and avg reward is -522.8022351521968\n",
      "total reward after 16850 steps is -387.1839749987285 and avg reward is -521.1677111528313\n",
      "total reward after 16900 steps is -453.44272968924963 and avg reward is -521.5333843692605\n"
     ]
    }
   ],
   "source": [
    "from Angrybird import AngryBird\n",
    "scale = np.array([10., 100., 50.])\n",
    "with tf.device('GPU:0'):\n",
    "    tf.random.set_seed(336699)\n",
    "    agent = Agent(2)\n",
    "    env = AngryBird()\n",
    "    episods = 20000\n",
    "    ep_reward = []\n",
    "    total_avgr = []\n",
    "    target = False\n",
    "\n",
    "    for s in range(episods):\n",
    "        if target == True:\n",
    "            break\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ## model of wing\n",
    "        cost = 0\n",
    "\n",
    "        while not done:\n",
    "            hand_made_state = state / scale\n",
    "            if state[0] == 11.:\n",
    "                action = np.zeros(2)\n",
    "            else:\n",
    "                action = agent.act(hand_made_state)\n",
    "            action_local = action + main(state[0], state[1:])\n",
    "            next_state, reward, done, _ = env.step(action_local)\n",
    "            hand_made_next_state = next_state / scale\n",
    "            reward *= -1\n",
    "            agent.savexp(hand_made_state, hand_made_next_state, action, done, reward)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        if done and s % 50 == 0:\n",
    "            ep_reward.append(total_reward)\n",
    "            avg_reward = np.mean(ep_reward[-100:])\n",
    "            total_avgr.append(avg_reward)\n",
    "            print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
    "            if int(avg_reward) < 0.1 and False:\n",
    "                target = True\n",
    "        if (s+1)%5000 == 0:\n",
    "            print(\"Saving model...\")\n",
    "            agent.actor_main.save_weights(\"td3_actor_{}\".format(s+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFoOf-MYf-Ep"
   },
   "outputs": [],
   "source": [
    "ep = [i  for i in range(len(total_avgr))]\n",
    "plt.plot( range(len(total_avgr)),total_avgr,'b')\n",
    "plt.title(\"Avg Test Aeward Vs Test Episods\")\n",
    "plt.xlabel(\"Test Episods\")\n",
    "\n",
    "plt.ylabel(\"Average Test Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(10):\n",
    "    hand_made_state = state / scale \n",
    "    action = agent.act(hand_made_state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(action, reward)\n",
    "    state = next_state\n",
    "traj = np.array(env.trajectoire)\n",
    "plt.plot(traj[:, 1], traj[:, 2])#, label=\"{}\".format())\n",
    "plt.scatter(traj[-1][1], traj[-1][2])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPy/vODWhRXFnIkwHUNOvhl",
   "include_colab_link": true,
   "name": "td3withtau.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
