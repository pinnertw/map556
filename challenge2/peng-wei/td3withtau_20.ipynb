{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class RBuffer():\n",
    "    def __init__(self, maxsize, statedim, naction):\n",
    "        self.cnt = 0\n",
    "        self.maxsize = maxsize\n",
    "        self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
    "\n",
    "    def reset(self):\n",
    "        self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
    "\n",
    "    def storexp(self, state, next_state, action, done, reward):\n",
    "        index = self.cnt % self.maxsize\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.done_memory[index] = 1- int(done)\n",
    "        self.cnt += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.cnt, self.maxsize)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace= False)\n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "        return states, next_states, rewards, actions, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "    def fit(self, X, Z, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype= tf.float32)\n",
    "        Z = tf.convert_to_tensor(Z, dtype= tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype= tf.float32)\n",
    "        a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self(X, Z)\n",
    "            cost = tf.keras.losses.MSE(Y, action)\n",
    "            grad = tape.gradient(cost, self.trainable_variables)\n",
    "            a_opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return tf.math.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()\n",
    "        initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=0.1)\n",
    "        self.f1 = tf.keras.layers.Dense(40, kernel_initializer=initializer, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(40, kernel_initializer=initializer, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation=None)\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)\n",
    "        return x\n",
    "    def fit(self, X, Y):\n",
    "        X = tf.convert_to_tensor(X, dtype= tf.float32)\n",
    "        Y = tf.convert_to_tensor(Y, dtype= tf.float32)\n",
    "        a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action = self(X)\n",
    "            cost = tf.keras.losses.MSE(Y, action)\n",
    "            grad = tape.gradient(cost, self.trainable_variables)\n",
    "            a_opt.apply_gradients(zip(grad, self.trainable_variables))\n",
    "        return tf.math.reduce_mean(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e"
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_action= 2):\n",
    "        self.actor_main = Actor(n_action)\n",
    "        self.actor_target = Actor(n_action)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_main2 = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target2 = Critic()\n",
    "        self.batch_size = 128\n",
    "        self.n_actions = 2\n",
    "        self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        # self.actor_target = tf.keras.optimizers.Adam(.001)\n",
    "        self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
    "        # self.critic_target = tf.keras.optimizers.Adam(.002)\n",
    "        self.memory = RBuffer(100000, [3], n_action)\n",
    "        self.trainstep = 0\n",
    "        #self.replace = 5\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = -100\n",
    "        self.max_action = 100\n",
    "        self.actor_update_steps = 20\n",
    "        self.warmup = 200\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt1)\n",
    "        self.critic_target2.compile(optimizer=self.c_opt2)\n",
    "        self.tau = 0.005\n",
    "\n",
    "    def savexp(self,state, next_state, action, done, reward):\n",
    "        self.memory.storexp(state, next_state, action, done, reward)\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "        weights3 = []\n",
    "        targets3 = self.critic_target2.weights\n",
    "        for i, weight in enumerate(self.critic_main2.weights):\n",
    "            weights3.append(weight * tau + targets3[i]*(1-tau))\n",
    "        self.critic_target2.set_weights(weights3)\n",
    "\n",
    "  \n",
    "    def train(self):\n",
    "        if self.memory.cnt < self.batch_size:\n",
    "            return\n",
    "\n",
    "        states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "        #dones = tf.convert_to_tensor(dones, dtype= tf.bool)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(next_states)\n",
    "            target_actions += tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = tf.clip_by_value(target_actions, self.min_action, self.max_action)\n",
    "\n",
    "\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(next_states, target_actions), 1)\n",
    "            target_next_state_values2 = tf.squeeze(self.critic_target2(next_states, target_actions), 1)\n",
    "\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            critic_value2 = tf.squeeze(self.critic_main2(states, actions), 1)\n",
    "\n",
    "            next_state_target_value = tf.math.minimum(target_next_state_values, target_next_state_values2)\n",
    "\n",
    "            target_values = rewards + self.gamma * next_state_target_value * dones\n",
    "            critic_loss1 = tf.keras.losses.MSE(target_values, critic_value)\n",
    "            critic_loss2 = tf.keras.losses.MSE(target_values, critic_value2)\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic_main2.trainable_variables)\n",
    "\n",
    "        self.c_opt1.apply_gradients(zip(grads1, self.critic_main.trainable_variables))\n",
    "        self.c_opt2.apply_gradients(zip(grads2, self.critic_main2.trainable_variables))\n",
    "\n",
    "\n",
    "        self.trainstep +=1\n",
    "\n",
    "        if self.trainstep % self.actor_update_steps == 0:\n",
    "\n",
    "            with tf.GradientTape() as tape3:\n",
    "\n",
    "                new_policy_actions = self.actor_main(states)\n",
    "                actor_loss = self.critic_main(states, new_policy_actions)\n",
    "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "            grads3 = tape3.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "            self.a_opt.apply_gradients(zip(grads3, self.actor_main.trainable_variables))\n",
    "\n",
    "        #if self.trainstep % self.replace == 0:\n",
    "        self.update_target()\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if self.trainstep > self.warmup:\n",
    "            evaluate = True\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
    "        return actions[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from Angrybird import AngryBird\n",
    "scale = np.array([10., 100., 50.])\n",
    "env = AngryBird()\n",
    "episods = 1000\n",
    "states = []\n",
    "actions = []\n",
    "costs = []\n",
    "for s in range(episods):\n",
    "    state = env.reset()\n",
    "    cost = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        hand_made_state = state / scale\n",
    "        if state[0] == 11.:\n",
    "            action = np.zeros(2)\n",
    "        else:\n",
    "            action = main(state[0], state[1:])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        # Update dataset\n",
    "        states.append(hand_made_state)\n",
    "        actions.append(action)\n",
    "        costs.append(reward)\n",
    "        state = next_state\n",
    "states = np.array(states)\n",
    "actions = np.array(actions)\n",
    "costs = np.array(costs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with tf.device('GPU:0'):\n",
    "    a = 50\n",
    "    while a > 1.25:\n",
    "        a = agent.actor_main.fit(states, actions).numpy()\n",
    "        agent.actor_target.fit(states, actions)\n",
    "        print(a, end=\" \")\n",
    "    a = 50\n",
    "    print()\n",
    "    while a > 0.01:\n",
    "        a = agent.critic_main.fit(states, actions, costs).numpy()\n",
    "        agent.critic_main2.fit(states, actions, costs).numpy()\n",
    "        agent.critic_target.fit(states, actions, costs).numpy()\n",
    "        agent.critic_target2.fit(states, actions, costs).numpy()\n",
    "        print(a, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(seconde, position):\n",
    "    return np.array([-(position[0]-10.*seconde),-(position[1]-(20.*seconde-2*seconde** 2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Angrybird import AngryBird\n",
    "scale = np.array([10., 100., 50.])\n",
    "with tf.device('GPU:0'):\n",
    "    tf.random.set_seed(336699)\n",
    "    agent = Agent(2)\n",
    "    env = AngryBird()\n",
    "    episods = 100000\n",
    "    ep_reward = []\n",
    "    total_avgr = []\n",
    "    target = False\n",
    "\n",
    "    for s in range(episods):\n",
    "        if target == True:\n",
    "            break\n",
    "        total_reward = 0\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ## model of wing\n",
    "        cost = 0\n",
    "\n",
    "        while not done:\n",
    "            hand_made_state = state / scale\n",
    "            if state[0] == 11.:\n",
    "                action = np.zeros(2)\n",
    "            else:\n",
    "                action = agent.act(hand_made_state)\n",
    "            action_local = action + main(state[0], state[1:])\n",
    "            next_state, reward, done, _ = env.step(action_local)\n",
    "            hand_made_next_state = next_state / scale\n",
    "            agent.savexp(hand_made_state, hand_made_next_state, action, done, reward)\n",
    "            agent.train()\n",
    "            #print(state, reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        if done and s % 50 == 0:\n",
    "            ep_reward.append(total_reward)\n",
    "            avg_reward = np.mean(ep_reward[-100:])\n",
    "            total_avgr.append(avg_reward)\n",
    "            print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
    "            if int(avg_reward) < 20:\n",
    "                target = True\n",
    "        if (s + 1) % 1000 == 0:\n",
    "            agent.actor_main.save_weights(\"td3_actor_{}\".format(s+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for weight in agent.actor_main.weights:\n",
    "    pass\n",
    "    #print(tf.Variable(weight.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFoOf-MYf-Ep"
   },
   "outputs": [],
   "source": [
    "ep = [i  for i in range(len(total_avgr))]\n",
    "plt.plot( range(len(total_avgr)),total_avgr,'b')\n",
    "plt.title(\"Avg Test Aeward Vs Test Episods\")\n",
    "plt.xlabel(\"Test Episods\")\n",
    "\n",
    "plt.ylabel(\"Average Test Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iffzAWwBELNL",
    "outputId": "07816cb3-31f4-4ed0-8817-09e8c7b8bd5e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for i in range(10):\n",
    "    hand_made_state = state / scale \n",
    "    action = agent.act(hand_made_state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    print(action, reward)\n",
    "    state = next_state\n",
    "traj = np.array(env.trajectoire)\n",
    "plt.plot(traj[:, 1], traj[:, 2])#, label=\"{}\".format())\n",
    "plt.scatter(traj[-1][1], traj[-1][2])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPy/vODWhRXFnIkwHUNOvhl",
   "include_colab_link": true,
   "name": "td3withtau.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
